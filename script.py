
import os
import sys
import logging
import pandas as pd
from notification import Notifier
from datetime import datetime, timedelta
from pathlib import Path
from dotenv import load_dotenv
from exchange_lib import get_exchange_account, find_and_download_emails
from extract_zippy import extract_all_zips
from processing_3G_Ericsson import Ericsson3GProcessor
from processing_3G_ZTE import ZTE3GProcessor
from processing_4G_Ericsson import Ericsson4GProcessor
from processing_4G_ZTE import ZTE4GProcessor
import shutil
import argparse

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env
load_dotenv()

# =================================================================
# ========== C·∫§U H√åNH - C√ì TH·ªÇ THAY ƒê·ªîI ·ªû ƒê√ÇY =====================
# =================================================================

# Th∆∞ m·ª•c t√¨m ki·∫øm (t√™n ch√≠nh x√°c trong Exchange)
FOLDER_NAME = "inbox"
FOLDER_NAME_Z = "inbox"

# Email ng∆∞·ªùi g·ª≠i (None = kh√¥ng l·ªçc theo ng∆∞·ªùi g·ª≠i)
SENDER_EMAIL = "bac.dx@vietnamobile.com.vn"
SENDER_EMAIL_Z = "vnm.performance.reporting@vietnamobile.com.vn"

# Danh s√°ch ti√™u ƒë·ªÅ email c·∫ßn t√¨m
LIST_OF_EMAILS = [
    "Automate_3G_Throughput",
    "Automate_3G_Traffic_User",
    "Automate_VoLTE_Traffic_Ericsson",
    "Automate_North_LTE_Traffic_Data",
    # Th√™m c√°c ti√™u ƒë·ªÅ email kh√°c v√†o ƒë√¢y
]

LIST_OF_EMAILS_Z = [
    "[EXTERNAL]Task name:Automate_3G_ZTE_Traffic_EMS1_WD",
    "[EXTERNAL]Task name:Automate_3G_ZTE_User_TP_EMS1_BH",
    "[EXTERNAL]Task name:Automate_4G_ZTE_Traffic_EMS1_WD",
    "[EXTERNAL]Task name:Automate_4G_ZTE_User_TP_EMS1_BH",
    "[EXTERNAL]Task name:Automate_3G_ZTE_Traffic_EMS2_WD",
    "[EXTERNAL]Task name:Automate_3G_ZTE_User_TP_EMS2_BH",
    "[EXTERNAL]Task name:Automate_4G_ZTE_Traffic_EMS2_WD",
    "[EXTERNAL]Task name:Automate_4G_ZTE_User_TP_EMS2_BH",
    # Th√™m c√°c ti√™u ƒë·ªÅ email kh√°c v√†o ƒë√¢y
]

# Th∆∞ m·ª•c l∆∞u file t·∫£i v·ªÅ
DOWNLOAD_FOLDER = "downloads"

# Th∆∞ m·ª•c sao ch√©p file k·∫øt qu·∫£ (ƒë·ªÉ tr·ªëng = kh√¥ng sao ch√©p)
# V√≠ d·ª•: PATH_TO_COPY = r"D:\Reports\Archive"
PATH_TO_COPY = r"D:/Project/Automate PowerBI - Display Site Information/Backup_Aggregate/"

# Ch·ªâ download c√°c ƒë·ªãnh d·∫°ng file n√†y (ƒë·ªÉ tr·ªëng = t·∫•t c·∫£ file)
# V√≠ d·ª•: [".xlsx", ".pdf", ".csv"]
ALLOWED_EXTENSIONS = []

# Th·ªùi gian t√¨m ki·∫øm - M·∫∑c ƒë·ªãnh l√† h√¥m nay
# Thay ƒë·ªïi th√†nh s·ªë ng√†y trong qu√° kh·ª© n·∫øu mu·ªën t√¨m email c≈© h∆°n
# V√≠ d·ª•: 1 = h√¥m qua, 7 = m·ªôt tu·∫ßn tr∆∞·ªõc
DAYS_TO_SEARCH = 0

# M·ª©c log: WARNING = √≠t th√¥ng b√°o, INFO = nhi·ªÅu th√¥ng b√°o h∆°n, DEBUG = r·∫•t chi ti·∫øt
LOG_LEVEL = logging.WARNING


# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=LOG_LEVEL)
logger = logging.getLogger(__name__)

# =================================================================
# =================== Ch∆∞∆°ng tr√¨nh ch√≠nh ==========================
# =================================================================

# ========== C·∫§U H√åNH LOGGING & EMAIL =============================
# =================================================================

# Email nh·∫≠n b√°o c√°o k·∫øt qu·∫£ v√† log ch·∫°y script chi ti·∫øt
RESULT_RECEIVER_LIST = [
    "bac.dx@vietnamobile.com.vn",
    # "thanh.tv@vietnamobile.com.vn",
    # Th√™m email ng∆∞·ªùi nh·∫≠n kh√°c v√†o ƒë√¢y
]

RESULT_EMAIL_SUBJECT = "[Automate Job Result]"

# =================================================================
# ========== LOGGER CLASS =========================================
# =================================================================

class Logger(object):
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "a", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()  # Ensure write to disk immediately

    def flush(self, *args, **kwargs):
        self.terminal.flush()
        self.log.flush()

def setup_logging(process_date):
    """Thi·∫øt l·∫≠p logging v√†o file"""
    log_dir = "Log"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        
    # T√™n file log theo ng√†y x·ª≠ l√Ω (process_date)
    log_filename = f"log_{process_date.strftime('%Y-%m-%d')}.txt"
    log_path = os.path.join(log_dir, log_filename)
    
    # Redirect stdout v√† stderr v√†o file log
    sys.stdout = Logger(log_path)
    sys.stderr = sys.stdout
    
    return log_path

# =================================================================
# ========== MAIN SCRIPT ==========================================
# =================================================================

def main():
    # Th√™m parser cho command-line arguments
    parser = argparse.ArgumentParser(description="Automated BI Report Generator.")
    parser.add_argument(
        "-s", "--skip-email",
        action="store_true",
        help="Skip email connection and download steps, process local files directly."
    )
    parser.add_argument(
        "-d", "--process-date",
        type=str,
        help="Specific date to process (YYYY-MM-DD). Defaults to yesterday if not provided."
    )
    args = parser.parse_args()

    # Determine processing date
    if args.process_date:
        try:
            process_date = datetime.strptime(args.process_date, "%Y-%m-%d")
        except ValueError:
            print("‚ùå Invalid date format. Please use YYYY-MM-DD.")
            return
    else:
        # Default to yesterday
        process_date = datetime.now() - timedelta(days=1)

    # Logic: D·ªØ li·ªáu c·ªßa ng√†y T (process_date) n·∫±m trong email g·ª≠i ng√†y T+1
    # Do ƒë√≥ ng√†y t√¨m ki·∫øm email ph·∫£i l√† process_date + 1 ng√†y
    email_search_date = process_date + timedelta(days=1)
    
    # 1. Setup Logging
    log_path = setup_logging(process_date)
    print(f"üìù Log file: {log_path}")
    print(f"üìÖ User Selected Data Date: {process_date.strftime('%Y-%m-%d')}")
    print(f"üìß Email Search Date: {email_search_date.strftime('%Y-%m-%d')}")
    print(f"üïí Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    if args.skip_email:
        print("\n" + "="*60)
        print("‚è≠Ô∏è  --skip-email flag detected. B·ªè qua c√°c b∆∞·ªõc download v√† gi·∫£i n√©n.")
        print("   ƒê·∫£m b·∫£o file ƒë√£ ƒë∆∞·ª£c gi·∫£i n√©n v√† s·∫µn s√†ng trong th∆∞ m·ª•c 'downloads'.")
        print("="*60 + "\n")
    
    account = None
    current_step = "Initialization"
    
    try:
        if not args.skip_email:
            # 2. K·∫øt n·ªëi Exchange
            current_step = "Connect Exchange"
            print("\n" + "="*60)
            print("üîå K·∫æT N·ªêI EXCHANGE SERVER")
            print("="*60 + "\n")
            account = get_exchange_account()

            if not account:
                raise Exception("Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Exchange Server")

            # 3. D·ªçn d·∫πp th∆∞ m·ª•c downloads
            current_step = "Clean Downloads"
            print("\n" + "="*60)
            print("üßπ D·ªåN D·∫∏P TH∆Ø M·ª§C")
            print("="*60 + "\n")
            
            if os.path.exists(DOWNLOAD_FOLDER):
                shutil.rmtree(DOWNLOAD_FOLDER)
                print(f"‚úÖ ƒê√£ x√≥a th∆∞ m·ª•c: {DOWNLOAD_FOLDER}")
            
            os.makedirs(DOWNLOAD_FOLDER)
            print(f"‚úÖ ƒê√£ t·∫°o l·∫°i th∆∞ m·ª•c: {DOWNLOAD_FOLDER}")

            # 4. T√¨m v√† download t·ª´ danh s√°ch subject c·ªßa m√¨nh
            current_step = "Download Ericsson KPIs (Personal)"
            print("\n" + "="*60)
            print("üì• T·∫¢I FILE T·ª™ EMAIL (Ericsson)")
            print("="*60 + "\n")
            
            results = find_and_download_emails(
                account=account,
                folder_name=FOLDER_NAME,
                sender_email=SENDER_EMAIL,
                subject_list=LIST_OF_EMAILS,
                download_folder=DOWNLOAD_FOLDER,
                days_back=DAYS_TO_SEARCH,
                allowed_extensions=ALLOWED_EXTENSIONS,
                target_date=email_search_date  # Pass email_search_date
            )

            # 5. Hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt (t√πy ch·ªçn)
            if results:
                print("\nüìã Chi ti·∫øt k·∫øt qu·∫£:")
                for subject, files in results.items():
                    if files:
                        print(f"  ‚úÖ {subject}: {len(files)} file")
                    else:
                        print(f"  ‚ùå {subject}: Kh√¥ng t√¨m th·∫•y file")

            # 6. T√¨m v√† download t·ª´ danh s√°ch subject c·ªßa Z
            current_step = "Download ZTE KPIs (Shared)"
            print("\n" + "="*60)
            print("üì• T·∫¢I FILE T·ª™ EMAIL (ZTE)")
            print("="*60 + "\n")
            
            results_z = find_and_download_emails(
                account=account,
                folder_name=FOLDER_NAME_Z,
                sender_email=SENDER_EMAIL_Z,
                subject_list=LIST_OF_EMAILS_Z,
                download_folder=DOWNLOAD_FOLDER,
                days_back=DAYS_TO_SEARCH,
                allowed_extensions=ALLOWED_EXTENSIONS,
                target_date=email_search_date  # Pass email_search_date
            )

            # 7. Hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt (t√πy ch·ªçn) c·ªßa Z
            if results_z:
                print("\nüìã Chi ti·∫øt k·∫øt qu·∫£:")
                for subject, files in results_z.items():
                    if files:
                        print(f"  ‚úÖ {subject}: {len(files)} file")
                    else:
                        print(f"  ‚ùå {subject}: Kh√¥ng t√¨m th·∫•y file")

        # 8. Gi·∫£i n√©n t·∫•t c·∫£ file ZIP trong th∆∞ m·ª•c downloads
        current_step = "Extract ZIPs"
        print("\n" + "="*60)
        print("üì¶ GI·∫¢I N√âN FILE ZIP")
        print("="*60 + "\n")
        extract_all_zips(DOWNLOAD_FOLDER)

        # 9. X·ª≠ l√Ω d·ªØ li·ªáu t·ª´ 4 processors
        current_step = "Data Processing"
        print("\n" + "="*60)
        print("üîÑ X·ª¨ L√ù D·ªÆ LI·ªÜU")
        print("="*60 + "\n")
        
        # Process 3G Ericsson
        current_step = "Processing 3G Ericsson"
        print("üìä Processing 3G Ericsson...")
        processor_3g_eric = Ericsson3GProcessor(download_folder=DOWNLOAD_FOLDER)
        processor_3g_eric.load_all_3g_data()
        processor_3g_eric.transform_all()
        processor_3g_eric.merge_final_result()
        processor_3g_eric.standardize_columns()
        processor_3g_eric.clean_data()
        df_3g_eric_site = processor_3g_eric.aggregate_by_site()
        print(f"‚úÖ 3G Ericsson: {len(df_3g_eric_site):,} sites\n")
        
        # Process 3G ZTE
        current_step = "Processing 3G ZTE"
        print("üìä Processing 3G ZTE...")
        processor_3g_zte = ZTE3GProcessor(download_folder=DOWNLOAD_FOLDER)
        processor_3g_zte.load_all_3g_zte_data()
        processor_3g_zte.merge_final_result()
        processor_3g_zte.standardize_columns()
        processor_3g_zte.clean_data()
        df_3g_zte_site = processor_3g_zte.aggregate_by_site()
        print(f"‚úÖ 3G ZTE: {len(df_3g_zte_site):,} sites\n")
        
        # Process 4G Ericsson
        current_step = "Processing 4G Ericsson"
        print("üìä Processing 4G Ericsson...")
        processor_4g_eric = Ericsson4GProcessor(download_folder=DOWNLOAD_FOLDER)
        processor_4g_eric.load_all_4g_ericsson_data()
        processor_4g_eric.merge_final_result()
        processor_4g_eric.standardize_columns()
        processor_4g_eric.clean_data()
        df_4g_eric_site = processor_4g_eric.aggregate_by_site()
        print(f"‚úÖ 4G Ericsson: {len(df_4g_eric_site):,} sites\n")
        
        # Process 4G ZTE
        current_step = "Processing 4G ZTE"
        print("üìä Processing 4G ZTE...")
        processor_4g_zte = ZTE4GProcessor(download_folder=DOWNLOAD_FOLDER)
        processor_4g_zte.load_all_4g_zte_data()
        processor_4g_zte.merge_final_result()
        processor_4g_zte.standardize_columns()
        processor_4g_zte.clean_data()
        df_4g_zte_site = processor_4g_zte.aggregate_by_site()
        print(f"‚úÖ 4G ZTE: {len(df_4g_zte_site):,} sites\n")
        
        # 10. Concat 3G data (Ericsson + ZTE)
        # Finished processing data for all files, now combine all data to final files
        print("\n" + "="*60)
        print("üîó CONCATENATING DATA 3G AND 4G")
        print("="*60 + "\n")
        current_step = "Concatenating 3G Data"
        print("üîó Concatenating 3G data...")
        df_3g_site = pd.concat([df_3g_eric_site, df_3g_zte_site], ignore_index=True)
        print(f"‚úÖ 3G Combined: {len(df_3g_site):,} sites\n")
        
        # 11. Concat 4G data (Ericsson + ZTE)
        current_step = "Concatenating 4G Data"
        print("üîó Concatenating 4G data...")
        df_4g_site = pd.concat([df_4g_eric_site, df_4g_zte_site], ignore_index=True)
        print(f"‚úÖ 4G Combined: {len(df_4g_site):,} sites\n")
        
        print("\n" + "="*60)
        print("üîó MERGING DATA 3G AND 4G TO ONE DATAFRAME")
        print("="*60 + "\n")
        # 12. Merge 3G + 4G data
        current_step = "Merging 3G + 4G Data"
        print("üîó Merging 3G + 4G data...")
        df_site_data = pd.merge(
            df_3g_site,
            df_4g_site,
            on='SiteID',
            how='outer'
        ).fillna(0)
        print(f"‚úÖ Site Data: {len(df_site_data):,} sites\n")
        
        # 13. Load SiteLocation and add location data
        # Skip this step - Long Lat will be added in another place

        # current_step = "Adding Location Data"
        # print("üìç Adding location data...")
        # site_location_path = Path(__file__).parent / "SiteLocation.csv"
        # df_location = pd.read_csv(site_location_path, usecols=['Site_ID', 'Long', 'Lat'])
        
        # Add Date column (based on process_date)
        df_site_data['Date'] = pd.to_datetime(process_date.strftime('%Y-%m-%d'))
        
        # Lookup Long and Lat
        # location_dict_long = dict(zip(df_location['Site_ID'], df_location['Long']))
        # location_dict_lat = dict(zip(df_location['Site_ID'], df_location['Lat']))
        
        # df_site_data['Long'] = df_site_data['SiteID'].map(location_dict_long).fillna(0)
        # df_site_data['Lat'] = df_site_data['SiteID'].map(location_dict_lat).fillna(0)
        
        # Reorder columns
        arranged_columns = [
            'Date', 'SiteID',
            '3G_User', '3G_Speed', '3G_Voice', '3G_Data',
            '4G_User', '4G_Speed', '4G_Voice', '4G_Data'
        ]
        df_site_data = df_site_data[arranged_columns]
        
        # Rename columns to final names
        final_column_names = {
            'SiteID': 'Site',
            '3G_User': '3G Sub',
            '3G_Speed': '3G Speed',
            '3G_Voice': '3G Voice traffic',
            '3G_Data': '3G Data traffic',
            '4G_User': '4G Sub',
            '4G_Speed': '4G Speed',
            '4G_Voice': '4G Voice traffic',
            '4G_Data': '4G Data traffic'
        }
        df_site_data = df_site_data.rename(columns=final_column_names)

        # print(f"‚úÖ Added location data\n")
        
        # 13.5 Drop rows with Long = 0 or Lat = 0
        # current_step = "Filtering Invalid Coordinates"
        # print("üó∫Ô∏è Filtering out sites with invalid coordinates...")
        # initial_count = len(df_site_data)
        # df_site_data = df_site_data[(df_site_data['Long'] != 0) & (df_site_data['Lat'] != 0)]
        # filtered_count = len(df_site_data)
        # dropped_count = initial_count - filtered_count
        # print(f"‚úÖ Filtered out {dropped_count:,} sites with Long=0 or Lat=0")
        
        # 14. Save to Aggregate.xlsx
        current_step = "Saving to Excel"
        print("üíæ Saving to Aggregate.xlsx...")
        aggregate_file = Path(__file__).parent.parent / "Aggregate.xlsx"
        print(f'Aggregate file path: {aggregate_file}')
        
        if aggregate_file.exists():
            # Load existing data
            df_existing = pd.read_excel(aggregate_file)
            
            # Append new data
            df_combined = pd.concat([df_existing, df_site_data], ignore_index=True)
            
            # Convert Date to datetime
            df_combined['Date'] = pd.to_datetime(df_combined['Date'])
            
            # Keep only last 30 days
            cutoff_date = datetime.now() - timedelta(days=30)
            df_combined = df_combined[df_combined['Date'] >= cutoff_date]
            
            # Sort by Date
            df_combined = df_combined.sort_values('Date')
            
            print(f"‚úÖ Appended to existing file")
            print(f"üìä Total records: {len(df_combined):,} (last 30 days)")
        else:
            df_combined = df_site_data
            print(f"‚úÖ Created new file")
            print(f"üìä Total records: {len(df_combined):,}")
        
        # Save to Excel
        df_combined.to_excel(aggregate_file, index=False)
        print(f"‚úÖ Saved to {aggregate_file.name}\n")
        
        # 14.5 Copy to external folder
        if PATH_TO_COPY:
            current_step = "Copying to external folder"
            try:
                print(f"üìÇ Copying to: {PATH_TO_COPY}")
                destination_dir = Path(PATH_TO_COPY)
                
                if not destination_dir.exists():
                    print(f"   Creating directory: {PATH_TO_COPY}")
                    destination_dir.mkdir(parents=True, exist_ok=True)
                
                destination_file = destination_dir / aggregate_file.name
                file_existed = destination_file.exists()
                
                shutil.copy2(aggregate_file, destination_file)
                
                if file_existed:
                    print(f"üîÑ ƒê√£ ghi ƒë√® file t·∫°i: {destination_file}")
                else:
                    print(f"‚úÖ Copy th√†nh c√¥ng file t·ªõi: {destination_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to copy file: {e}")
                # Don't stop the process, just log warning
        
        print("="*60)
        print("üéâ DATA PROCESSING COMPLETED!")
        print("="*60)
        
        # 15. G·ª≠i email th√¥ng b√°o th√†nh c√¥ng
        if account:
            notifier = Notifier(account, RESULT_RECEIVER_LIST, RESULT_EMAIL_SUBJECT)
            notifier.send_success(log_file=log_path)

    except Exception as e:
        print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG T·∫†I B∆Ø·ªöC: {current_step}")
        print(f"‚ùå Error details: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # G·ª≠i email th√¥ng b√°o l·ªói
        if account:
            notifier = Notifier(account, RESULT_RECEIVER_LIST, RESULT_EMAIL_SUBJECT)
            notifier.send_failure(step_name=current_step, error_msg=str(e), log_file=log_path)
        else:
            try:
                # Ch·ªâ th·ª≠ k·∫øt n·ªëi l·∫°i n·∫øu ban ƒë·∫ßu ch∆∞a c√≥ account v√† kh√¥ng ph·∫£i do skip_email
                if not args.skip_email:
                    account = get_exchange_account() # try to connect to exchange server again
                    if account:
                        notifier = Notifier(account, RESULT_RECEIVER_LIST, RESULT_EMAIL_SUBJECT)
                        notifier.send_failure(step_name=current_step, error_msg=str(e), log_file=log_path)
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ g·ª≠i email b√°o l·ªói, l√Ω do: {str(e)}")

if __name__ == "__main__":
    main()